# Project: Intelligent Platform for AI Model Management and Information Retrieval

## General Description:
This project is an application designed to enhance the use of local and cloud-based Large Language Models (LLMs), including OpenAI and Grok, integrated with a Retrieval-Augmented Generation (RAG) system. It offers a user-friendly interface developed in Streamlit, with advanced capabilities for:

 - **Managing AI models through a dynamic selector.**

 - **Incorporating vector databases for efficient information storage and retrieval.**

 - **Ensuring security and ease of use with secure tunneling and customizable configurations.**

This app helps experiment with various models to determine the best action based on resources, model choice, budget, and cloud services.

## Key Features

1. **Main Framework**: 
   - **Pure Python** along with **Streamlit** for the user-friendly interface.

2. **Databases**:
   - **Firebase**:To store email validation in the account page.
   - **Chromadb**: Vector databases that store embeddings represented as numeric vectors.

3. **Models**:
   - **OpenAI Embeddings**: Vector database for embeddings generated by OpenAI.
   - **Ollama Embeddings**: Embeddings generated by local Ollama models.

4. ðŸ”’ **Secure Tunneling**:
   - **Ngrok**:  Creation of a secure tunnel (puerto `4040`) to share the application without vulnerabilities.

5. **Model Selector**:
   - **Option to choose from:**
     - OpenAI
     - Grok
     - Custom local models.

6. **Customization**:
   - **Easy update of the Ngrok token in Ngrok** `ngrok.yml`file.
   - **Selection of default models from the** `.env`file.

## Configuration

1. **Configure environment variables**
   - Edit the  `.env` file with the following content:
     ```env
     MODEL_LIST=Meta models that suit you
     ```

2. **Configure Ngrok:**
   - Edith the archivo `ngrok.yml` with your token:
     ```yaml
     authtoken: YOUR_NGROK_TOKEN
     ```
